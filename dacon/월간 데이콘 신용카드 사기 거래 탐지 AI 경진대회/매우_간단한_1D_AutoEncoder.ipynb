{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://dacon.io/en/codeshare/5508?dtype=recent"
      ],
      "metadata": {
        "id": "E2uFZSF8I8nI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DpXRZDBJ-_NX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kIk1KwCCkBi",
        "outputId": "5ee6f813-7001-44a4-cbce-477d5e2eb55e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq \"/content/gdrive/MyDrive/이상치 탐지 사기 (1).zip\""
      ],
      "metadata": {
        "id": "hLxFy5HnCy5n"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('./train.csv')\n",
        "test = pd.read_csv('./test.csv')"
      ],
      "metadata": {
        "id": "Bgv26R1AIMno"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "sGcq0uNLIORo",
        "outputId": "07fcf658-6e18-4016-b9fb-29da76d7bf90"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            ID         V1         V2        V3        V4        V5        V6  \\\n",
              "0            3  -1.358354  -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
              "1            4  -0.966272  -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
              "2            6  -0.425966   0.960523  1.141109 -0.168252  0.420987 -0.029728   \n",
              "3            8  -0.644269   1.417964  1.074380 -0.492199  0.948934  0.428118   \n",
              "4            9  -0.894286   0.286157 -0.113192 -0.271526  2.669599  3.721818   \n",
              "...        ...        ...        ...       ...       ...       ...       ...   \n",
              "113837  284796 -12.516732  10.187818 -8.476671 -2.510473 -4.586669 -1.394465   \n",
              "113838  284797   1.884849  -0.143540 -0.999943  1.506772 -0.035300 -0.613638   \n",
              "113839  284798  -0.241923   0.712247  0.399806 -0.463406  0.244531 -1.343668   \n",
              "113840  284802   0.120316   0.931005 -0.546012 -0.745097  1.130314 -0.235973   \n",
              "113841  284803 -11.881118  10.071785 -9.834783 -2.066656 -5.364473 -2.606837   \n",
              "\n",
              "              V7        V8        V9  ...       V21       V22       V23  \\\n",
              "0       0.791461  0.247676 -1.514654  ...  0.247998  0.771679  0.909412   \n",
              "1       0.237609  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321   \n",
              "2       0.476201  0.260314 -0.568671  ... -0.208254 -0.559825 -0.026398   \n",
              "3       1.120631 -3.807864  0.615375  ...  1.943465 -1.015455  0.057504   \n",
              "4       0.370145  0.851084 -0.392048  ... -0.073425 -0.268092 -0.204233   \n",
              "...          ...       ...       ...  ...       ...       ...       ...   \n",
              "113837 -3.632516  5.498583  4.893089  ... -0.944759 -1.565026  0.890675   \n",
              "113838  0.190241 -0.249058  0.666458  ...  0.144008  0.634646 -0.042114   \n",
              "113839  0.929369 -0.206210  0.106234  ... -0.228876 -0.514376  0.279598   \n",
              "113840  0.812722  0.115093 -0.204064  ... -0.314205 -0.808520  0.050343   \n",
              "113841 -4.918215  7.305334  1.914428  ...  0.213454  0.111864  1.014480   \n",
              "\n",
              "             V24       V25       V26       V27       V28       V29       V30  \n",
              "0      -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  4.983721 -0.994972  \n",
              "1      -1.175575  0.647376 -0.221929  0.062723  0.061458  1.418291 -0.994972  \n",
              "2      -0.371427 -0.232794  0.105915  0.253844  0.081080 -0.256131 -0.994960  \n",
              "3      -0.649709 -0.415267 -0.051634 -1.206921 -1.085339  0.262698 -0.994901  \n",
              "4       1.011592  0.373205 -0.384157  0.011747  0.142404  0.994900 -0.994901  \n",
              "...          ...       ...       ...       ...       ...       ...       ...  \n",
              "113837 -1.253276  1.786717  0.320763  2.090712  1.232864 -0.169496  1.034857  \n",
              "113838 -0.053206  0.316403 -0.461441  0.018265 -0.041068  0.530986  1.034881  \n",
              "113839  0.371441 -0.559238  0.113144  0.131507  0.081265 -0.230699  1.034904  \n",
              "113840  0.102800 -0.435870  0.124079  0.217940  0.068803 -0.269825  1.034939  \n",
              "113841 -0.509348  1.436807  0.250034  0.943651  0.823731 -0.296653  1.034951  \n",
              "\n",
              "[113842 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ee54d751-3a18-4ed7-887d-d0052725c258\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>...</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>V29</th>\n",
              "      <th>V30</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>...</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>4.983721</td>\n",
              "      <td>-0.994972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>1.418291</td>\n",
              "      <td>-0.994972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6</td>\n",
              "      <td>-0.425966</td>\n",
              "      <td>0.960523</td>\n",
              "      <td>1.141109</td>\n",
              "      <td>-0.168252</td>\n",
              "      <td>0.420987</td>\n",
              "      <td>-0.029728</td>\n",
              "      <td>0.476201</td>\n",
              "      <td>0.260314</td>\n",
              "      <td>-0.568671</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.208254</td>\n",
              "      <td>-0.559825</td>\n",
              "      <td>-0.026398</td>\n",
              "      <td>-0.371427</td>\n",
              "      <td>-0.232794</td>\n",
              "      <td>0.105915</td>\n",
              "      <td>0.253844</td>\n",
              "      <td>0.081080</td>\n",
              "      <td>-0.256131</td>\n",
              "      <td>-0.994960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8</td>\n",
              "      <td>-0.644269</td>\n",
              "      <td>1.417964</td>\n",
              "      <td>1.074380</td>\n",
              "      <td>-0.492199</td>\n",
              "      <td>0.948934</td>\n",
              "      <td>0.428118</td>\n",
              "      <td>1.120631</td>\n",
              "      <td>-3.807864</td>\n",
              "      <td>0.615375</td>\n",
              "      <td>...</td>\n",
              "      <td>1.943465</td>\n",
              "      <td>-1.015455</td>\n",
              "      <td>0.057504</td>\n",
              "      <td>-0.649709</td>\n",
              "      <td>-0.415267</td>\n",
              "      <td>-0.051634</td>\n",
              "      <td>-1.206921</td>\n",
              "      <td>-1.085339</td>\n",
              "      <td>0.262698</td>\n",
              "      <td>-0.994901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9</td>\n",
              "      <td>-0.894286</td>\n",
              "      <td>0.286157</td>\n",
              "      <td>-0.113192</td>\n",
              "      <td>-0.271526</td>\n",
              "      <td>2.669599</td>\n",
              "      <td>3.721818</td>\n",
              "      <td>0.370145</td>\n",
              "      <td>0.851084</td>\n",
              "      <td>-0.392048</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.073425</td>\n",
              "      <td>-0.268092</td>\n",
              "      <td>-0.204233</td>\n",
              "      <td>1.011592</td>\n",
              "      <td>0.373205</td>\n",
              "      <td>-0.384157</td>\n",
              "      <td>0.011747</td>\n",
              "      <td>0.142404</td>\n",
              "      <td>0.994900</td>\n",
              "      <td>-0.994901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113837</th>\n",
              "      <td>284796</td>\n",
              "      <td>-12.516732</td>\n",
              "      <td>10.187818</td>\n",
              "      <td>-8.476671</td>\n",
              "      <td>-2.510473</td>\n",
              "      <td>-4.586669</td>\n",
              "      <td>-1.394465</td>\n",
              "      <td>-3.632516</td>\n",
              "      <td>5.498583</td>\n",
              "      <td>4.893089</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.944759</td>\n",
              "      <td>-1.565026</td>\n",
              "      <td>0.890675</td>\n",
              "      <td>-1.253276</td>\n",
              "      <td>1.786717</td>\n",
              "      <td>0.320763</td>\n",
              "      <td>2.090712</td>\n",
              "      <td>1.232864</td>\n",
              "      <td>-0.169496</td>\n",
              "      <td>1.034857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113838</th>\n",
              "      <td>284797</td>\n",
              "      <td>1.884849</td>\n",
              "      <td>-0.143540</td>\n",
              "      <td>-0.999943</td>\n",
              "      <td>1.506772</td>\n",
              "      <td>-0.035300</td>\n",
              "      <td>-0.613638</td>\n",
              "      <td>0.190241</td>\n",
              "      <td>-0.249058</td>\n",
              "      <td>0.666458</td>\n",
              "      <td>...</td>\n",
              "      <td>0.144008</td>\n",
              "      <td>0.634646</td>\n",
              "      <td>-0.042114</td>\n",
              "      <td>-0.053206</td>\n",
              "      <td>0.316403</td>\n",
              "      <td>-0.461441</td>\n",
              "      <td>0.018265</td>\n",
              "      <td>-0.041068</td>\n",
              "      <td>0.530986</td>\n",
              "      <td>1.034881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113839</th>\n",
              "      <td>284798</td>\n",
              "      <td>-0.241923</td>\n",
              "      <td>0.712247</td>\n",
              "      <td>0.399806</td>\n",
              "      <td>-0.463406</td>\n",
              "      <td>0.244531</td>\n",
              "      <td>-1.343668</td>\n",
              "      <td>0.929369</td>\n",
              "      <td>-0.206210</td>\n",
              "      <td>0.106234</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.228876</td>\n",
              "      <td>-0.514376</td>\n",
              "      <td>0.279598</td>\n",
              "      <td>0.371441</td>\n",
              "      <td>-0.559238</td>\n",
              "      <td>0.113144</td>\n",
              "      <td>0.131507</td>\n",
              "      <td>0.081265</td>\n",
              "      <td>-0.230699</td>\n",
              "      <td>1.034904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113840</th>\n",
              "      <td>284802</td>\n",
              "      <td>0.120316</td>\n",
              "      <td>0.931005</td>\n",
              "      <td>-0.546012</td>\n",
              "      <td>-0.745097</td>\n",
              "      <td>1.130314</td>\n",
              "      <td>-0.235973</td>\n",
              "      <td>0.812722</td>\n",
              "      <td>0.115093</td>\n",
              "      <td>-0.204064</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.314205</td>\n",
              "      <td>-0.808520</td>\n",
              "      <td>0.050343</td>\n",
              "      <td>0.102800</td>\n",
              "      <td>-0.435870</td>\n",
              "      <td>0.124079</td>\n",
              "      <td>0.217940</td>\n",
              "      <td>0.068803</td>\n",
              "      <td>-0.269825</td>\n",
              "      <td>1.034939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113841</th>\n",
              "      <td>284803</td>\n",
              "      <td>-11.881118</td>\n",
              "      <td>10.071785</td>\n",
              "      <td>-9.834783</td>\n",
              "      <td>-2.066656</td>\n",
              "      <td>-5.364473</td>\n",
              "      <td>-2.606837</td>\n",
              "      <td>-4.918215</td>\n",
              "      <td>7.305334</td>\n",
              "      <td>1.914428</td>\n",
              "      <td>...</td>\n",
              "      <td>0.213454</td>\n",
              "      <td>0.111864</td>\n",
              "      <td>1.014480</td>\n",
              "      <td>-0.509348</td>\n",
              "      <td>1.436807</td>\n",
              "      <td>0.250034</td>\n",
              "      <td>0.943651</td>\n",
              "      <td>0.823731</td>\n",
              "      <td>-0.296653</td>\n",
              "      <td>1.034951</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>113842 rows × 31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ee54d751-3a18-4ed7-887d-d0052725c258')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ee54d751-3a18-4ed7-887d-d0052725c258 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ee54d751-3a18-4ed7-887d-d0052725c258');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "3TxeM91_IO7p",
        "outputId": "0a862e60-4b39-45b5-88e5-cc774c530922"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             ID        V1        V2        V3        V4        V5        V6  \\\n",
              "0       AAAA0x1 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
              "1       AAAA0x2  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
              "2       AAAA0x5 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
              "3       AAAA0x7  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708   \n",
              "4       AAAA0xc  0.384978  0.616109 -0.874300 -0.094019  2.924584  3.317027   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "142498  0x4587f  0.219529  0.881246 -0.635891  0.960928 -0.152971 -1.014307   \n",
              "142499  0x45880 -1.775135 -0.004235  1.189786  0.331096  1.196063  5.519980   \n",
              "142500  0x45884 -0.732789 -0.055080  2.035030 -0.738589  0.868229  1.058415   \n",
              "142501  0x45885  1.919565 -0.301254 -3.249640 -0.557828  2.630515  3.031260   \n",
              "142502  0x45887 -0.533413 -0.189733  0.703337 -0.506271 -0.012546 -0.649617   \n",
              "\n",
              "              V7        V8        V9  ...       V21       V22       V23  \\\n",
              "0       0.239599  0.098698  0.363787  ... -0.018307  0.277838 -0.110474   \n",
              "1      -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288   \n",
              "2       0.592941 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458   \n",
              "3      -0.005159  0.081213  0.464960  ... -0.167716 -0.270710 -0.154104   \n",
              "4       0.470455  0.538247 -0.558895  ...  0.049924  0.238422  0.009130   \n",
              "...          ...       ...       ...  ...       ...       ...       ...   \n",
              "142498  0.427126  0.121340 -0.285670  ...  0.099936  0.337120  0.251791   \n",
              "142499 -1.518185  2.080825  1.159498  ...  0.103302  0.654850 -0.348929   \n",
              "142500  0.024330  0.294869  0.584800  ...  0.214205  0.924384  0.012463   \n",
              "142501 -0.296827  0.708417  0.432454  ...  0.232045  0.578229 -0.037501   \n",
              "142502  1.577006 -0.414650  0.486180  ...  0.261057  0.643078  0.376777   \n",
              "\n",
              "             V24       V25       V26       V27       V28       V29       V30  \n",
              "0       0.066928  0.128539 -0.189115  0.133558 -0.021053  1.783274 -0.994983  \n",
              "1      -0.339846  0.167170  0.125895 -0.008983  0.014724 -0.269825 -0.994983  \n",
              "2       0.141267 -0.206010  0.502292  0.219422  0.215153  0.670579 -0.994960  \n",
              "3      -0.780055  0.750137 -0.257237  0.034507  0.005168 -0.237686 -0.994937  \n",
              "4       0.996710 -0.767315 -0.492208  0.042472 -0.054337 -0.167819 -0.994866  \n",
              "...          ...       ...       ...       ...       ...       ...       ...  \n",
              "142498  0.057688 -1.508368  0.144023  0.181205  0.215243  0.028645  1.034904  \n",
              "142499  0.745323  0.704545 -0.127579  0.454379  0.130308  0.810312  1.034916  \n",
              "142500 -1.016226 -0.606624 -0.395255  0.068472 -0.053527  0.038986  1.034963  \n",
              "142501  0.640134  0.265745 -0.087371  0.004455 -0.026561  0.641096  1.034975  \n",
              "142502  0.008797 -0.473649 -0.818267 -0.002415  0.013649  2.724796  1.035022  \n",
              "\n",
              "[142503 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e594887b-2a71-4b02-8703-19332ec08f94\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>...</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>V29</th>\n",
              "      <th>V30</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AAAA0x1</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>1.783274</td>\n",
              "      <td>-0.994983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AAAA0x2</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>-0.269825</td>\n",
              "      <td>-0.994983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AAAA0x5</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>0.670579</td>\n",
              "      <td>-0.994960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AAAA0x7</td>\n",
              "      <td>1.229658</td>\n",
              "      <td>0.141004</td>\n",
              "      <td>0.045371</td>\n",
              "      <td>1.202613</td>\n",
              "      <td>0.191881</td>\n",
              "      <td>0.272708</td>\n",
              "      <td>-0.005159</td>\n",
              "      <td>0.081213</td>\n",
              "      <td>0.464960</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.167716</td>\n",
              "      <td>-0.270710</td>\n",
              "      <td>-0.154104</td>\n",
              "      <td>-0.780055</td>\n",
              "      <td>0.750137</td>\n",
              "      <td>-0.257237</td>\n",
              "      <td>0.034507</td>\n",
              "      <td>0.005168</td>\n",
              "      <td>-0.237686</td>\n",
              "      <td>-0.994937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AAAA0xc</td>\n",
              "      <td>0.384978</td>\n",
              "      <td>0.616109</td>\n",
              "      <td>-0.874300</td>\n",
              "      <td>-0.094019</td>\n",
              "      <td>2.924584</td>\n",
              "      <td>3.317027</td>\n",
              "      <td>0.470455</td>\n",
              "      <td>0.538247</td>\n",
              "      <td>-0.558895</td>\n",
              "      <td>...</td>\n",
              "      <td>0.049924</td>\n",
              "      <td>0.238422</td>\n",
              "      <td>0.009130</td>\n",
              "      <td>0.996710</td>\n",
              "      <td>-0.767315</td>\n",
              "      <td>-0.492208</td>\n",
              "      <td>0.042472</td>\n",
              "      <td>-0.054337</td>\n",
              "      <td>-0.167819</td>\n",
              "      <td>-0.994866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142498</th>\n",
              "      <td>0x4587f</td>\n",
              "      <td>0.219529</td>\n",
              "      <td>0.881246</td>\n",
              "      <td>-0.635891</td>\n",
              "      <td>0.960928</td>\n",
              "      <td>-0.152971</td>\n",
              "      <td>-1.014307</td>\n",
              "      <td>0.427126</td>\n",
              "      <td>0.121340</td>\n",
              "      <td>-0.285670</td>\n",
              "      <td>...</td>\n",
              "      <td>0.099936</td>\n",
              "      <td>0.337120</td>\n",
              "      <td>0.251791</td>\n",
              "      <td>0.057688</td>\n",
              "      <td>-1.508368</td>\n",
              "      <td>0.144023</td>\n",
              "      <td>0.181205</td>\n",
              "      <td>0.215243</td>\n",
              "      <td>0.028645</td>\n",
              "      <td>1.034904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142499</th>\n",
              "      <td>0x45880</td>\n",
              "      <td>-1.775135</td>\n",
              "      <td>-0.004235</td>\n",
              "      <td>1.189786</td>\n",
              "      <td>0.331096</td>\n",
              "      <td>1.196063</td>\n",
              "      <td>5.519980</td>\n",
              "      <td>-1.518185</td>\n",
              "      <td>2.080825</td>\n",
              "      <td>1.159498</td>\n",
              "      <td>...</td>\n",
              "      <td>0.103302</td>\n",
              "      <td>0.654850</td>\n",
              "      <td>-0.348929</td>\n",
              "      <td>0.745323</td>\n",
              "      <td>0.704545</td>\n",
              "      <td>-0.127579</td>\n",
              "      <td>0.454379</td>\n",
              "      <td>0.130308</td>\n",
              "      <td>0.810312</td>\n",
              "      <td>1.034916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142500</th>\n",
              "      <td>0x45884</td>\n",
              "      <td>-0.732789</td>\n",
              "      <td>-0.055080</td>\n",
              "      <td>2.035030</td>\n",
              "      <td>-0.738589</td>\n",
              "      <td>0.868229</td>\n",
              "      <td>1.058415</td>\n",
              "      <td>0.024330</td>\n",
              "      <td>0.294869</td>\n",
              "      <td>0.584800</td>\n",
              "      <td>...</td>\n",
              "      <td>0.214205</td>\n",
              "      <td>0.924384</td>\n",
              "      <td>0.012463</td>\n",
              "      <td>-1.016226</td>\n",
              "      <td>-0.606624</td>\n",
              "      <td>-0.395255</td>\n",
              "      <td>0.068472</td>\n",
              "      <td>-0.053527</td>\n",
              "      <td>0.038986</td>\n",
              "      <td>1.034963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142501</th>\n",
              "      <td>0x45885</td>\n",
              "      <td>1.919565</td>\n",
              "      <td>-0.301254</td>\n",
              "      <td>-3.249640</td>\n",
              "      <td>-0.557828</td>\n",
              "      <td>2.630515</td>\n",
              "      <td>3.031260</td>\n",
              "      <td>-0.296827</td>\n",
              "      <td>0.708417</td>\n",
              "      <td>0.432454</td>\n",
              "      <td>...</td>\n",
              "      <td>0.232045</td>\n",
              "      <td>0.578229</td>\n",
              "      <td>-0.037501</td>\n",
              "      <td>0.640134</td>\n",
              "      <td>0.265745</td>\n",
              "      <td>-0.087371</td>\n",
              "      <td>0.004455</td>\n",
              "      <td>-0.026561</td>\n",
              "      <td>0.641096</td>\n",
              "      <td>1.034975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142502</th>\n",
              "      <td>0x45887</td>\n",
              "      <td>-0.533413</td>\n",
              "      <td>-0.189733</td>\n",
              "      <td>0.703337</td>\n",
              "      <td>-0.506271</td>\n",
              "      <td>-0.012546</td>\n",
              "      <td>-0.649617</td>\n",
              "      <td>1.577006</td>\n",
              "      <td>-0.414650</td>\n",
              "      <td>0.486180</td>\n",
              "      <td>...</td>\n",
              "      <td>0.261057</td>\n",
              "      <td>0.643078</td>\n",
              "      <td>0.376777</td>\n",
              "      <td>0.008797</td>\n",
              "      <td>-0.473649</td>\n",
              "      <td>-0.818267</td>\n",
              "      <td>-0.002415</td>\n",
              "      <td>0.013649</td>\n",
              "      <td>2.724796</td>\n",
              "      <td>1.035022</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>142503 rows × 31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e594887b-2a71-4b02-8703-19332ec08f94')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e594887b-2a71-4b02-8703-19332ec08f94 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e594887b-2a71-4b02-8703-19332ec08f94');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "cGiSsm5bIQAp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "metadata": {
        "id": "qjPM3QFDSTnt"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 400\n",
        "LR = 1e-2\n",
        "BS = 16384\n",
        "SEED = 41"
      ],
      "metadata": {
        "id": "N84PYGceIfiw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(SEED) # Seed 고정"
      ],
      "metadata": {
        "id": "Y_bQri01Igqg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('./train.csv')\n",
        "train_df = train_df.drop(columns=['ID'])\n",
        "val_df = pd.read_csv('./val.csv')\n",
        "val_df = val_df.drop(columns=['ID'])"
      ],
      "metadata": {
        "id": "UFsYYkIrIiOo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQkxDnxPJF6Q",
        "outputId": "59b98f60-75f9-45f4-f387-153caadd0783"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 113842 entries, 0 to 113841\n",
            "Data columns (total 30 columns):\n",
            " #   Column  Non-Null Count   Dtype  \n",
            "---  ------  --------------   -----  \n",
            " 0   V1      113842 non-null  float64\n",
            " 1   V2      113842 non-null  float64\n",
            " 2   V3      113842 non-null  float64\n",
            " 3   V4      113842 non-null  float64\n",
            " 4   V5      113842 non-null  float64\n",
            " 5   V6      113842 non-null  float64\n",
            " 6   V7      113842 non-null  float64\n",
            " 7   V8      113842 non-null  float64\n",
            " 8   V9      113842 non-null  float64\n",
            " 9   V10     113842 non-null  float64\n",
            " 10  V11     113842 non-null  float64\n",
            " 11  V12     113842 non-null  float64\n",
            " 12  V13     113842 non-null  float64\n",
            " 13  V14     113842 non-null  float64\n",
            " 14  V15     113842 non-null  float64\n",
            " 15  V16     113842 non-null  float64\n",
            " 16  V17     113842 non-null  float64\n",
            " 17  V18     113842 non-null  float64\n",
            " 18  V19     113842 non-null  float64\n",
            " 19  V20     113842 non-null  float64\n",
            " 20  V21     113842 non-null  float64\n",
            " 21  V22     113842 non-null  float64\n",
            " 22  V23     113842 non-null  float64\n",
            " 23  V24     113842 non-null  float64\n",
            " 24  V25     113842 non-null  float64\n",
            " 25  V26     113842 non-null  float64\n",
            " 26  V27     113842 non-null  float64\n",
            " 27  V28     113842 non-null  float64\n",
            " 28  V29     113842 non-null  float64\n",
            " 29  V30     113842 non-null  float64\n",
            "dtypes: float64(30)\n",
            "memory usage: 26.1 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class MyDataset(Dataset):\n",
        "#   def __init__(self,df,eval_mode):\n",
        "#     self.df = df\n",
        "#     self.eval_mode = eval_mode\n",
        "#     if self.eval_mode:\n",
        "#       self.labels = self.df['Class'].values\n",
        "#       self.df = self.df.drop(columns=['Class']).values\n",
        "#     else:\n",
        "#       self.df = self.df.values\n",
        "  \n",
        "#   def __getitem__(self,index):\n",
        "#     if self.eval_mode:\n",
        "#       self.x = self.df[index]\n",
        "#       self.y = self.label[index]\n",
        "#       return torch.Tensor(self.x), self.y\n",
        "\n",
        "#     else:\n",
        "#       self.x = self.df[index]\n",
        "#       return torch.Tensor(self.x)\n",
        "  \n",
        "#   def __len__(self):\n",
        "#     return len(self.df)\n"
      ],
      "metadata": {
        "id": "LCzqxUoqIlMw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, df, eval_mode):\n",
        "        self.df = df\n",
        "        self.eval_mode = eval_mode\n",
        "        if self.eval_mode:\n",
        "            self.labels = self.df['Class'].values\n",
        "            self.df = self.df.drop(columns=['Class']).values\n",
        "        else:\n",
        "            self.df = self.df.values\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        if self.eval_mode:\n",
        "            self.x = self.df[index]\n",
        "            self.y = self.labels[index]\n",
        "            return torch.Tensor(self.x), self.y\n",
        "        else:\n",
        "            self.x = self.df[index]\n",
        "            return torch.Tensor(self.x)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.df)"
      ],
      "metadata": {
        "id": "PLY068xxSfud"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MyDataset(df=train_df, eval_mode=False)"
      ],
      "metadata": {
        "id": "NG2hO9ibKnuP"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=BS, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "9KJ6-meSKxOW"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = MyDataset(df = val_df, eval_mode=True)"
      ],
      "metadata": {
        "id": "89XnWseZK45e"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader = DataLoader(val_dataset, batch_size=BS, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "aiMLfbZ_LD4I"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 내가 한거 실행이 안됨.\n",
        "# class AutoEncoder(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(AutoEncoder, self).__init__()\n",
        "#         self.Ecoder = nn.sequential(\n",
        "#             nn.Linear(30,64),\n",
        "#             nn.BatchNorm1d(64),\n",
        "#             nn.LeakyReLU(),\n",
        "#             nn.Linear(64,128),\n",
        "#             nn.BatchNorm1d(128),\n",
        "#             nn.LeakyReLU()\n",
        "#         )\n",
        "#         self.Decoder = nn.Sequential(\n",
        "#             nn.Linear(128,64),\n",
        "#             nn.BatchNorm1d(64),\n",
        "#             nn.LeakyReLU(),\n",
        "#             nn.Linear(64,30),\n",
        "#         )\n",
        "\n",
        "#     def forward(self,x):\n",
        "#       x = self.Encoder(x)\n",
        "#       x = self.Decoder(x)\n",
        "#       return x"
      ],
      "metadata": {
        "id": "y26c--xYLGOQ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AutoEncoder, self).__init__()\n",
        "        self.Encoder = nn.Sequential(\n",
        "            nn.Linear(30,64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(64,128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.LeakyReLU(),\n",
        "        )\n",
        "        self.Decoder = nn.Sequential(\n",
        "            nn.Linear(128,64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(64,30),\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.Encoder(x)\n",
        "        x = self.Decoder(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "AjFoUfRDRZg3"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class Trainer():\n",
        "#     def __init__(self, model, optimizer, train_loader, val_loader, scheduler, device):\n",
        "#       self.model=model\n",
        "#       self.optimizer = optimizer\n",
        "#       self.train_loader = train_loader\n",
        "#       self.val_loader = val_loader\n",
        "#       self.scheduler = scheduler\n",
        "#       self.device = device\n",
        "#       self.criterion = nn.L1Loss().to(self.device)\n",
        "  \n",
        "#     def fit(self, ):\n",
        "#       self.model.to(self.device)\n",
        "#       best_score=0\n",
        "#       for epoch in range(EPOCHS):\n",
        "#         self.model.train()\n",
        "#         train_loss = []\n",
        "#         for x in iter(self.train_loader):\n",
        "#           x = x.float().to(self.device)\n",
        "#           self.optimizer.zero_grad()\n",
        "\n",
        "#           _x = self.model(x)\n",
        "#           loss = self.criterion(x,_x)\n",
        "\n",
        "#           train_loss.append(loss.item())\n",
        "\n",
        "#       score = self.validation(self.model, 0.95)\n",
        "#       print(f'Epoch : [{epoch}] Train loss : [{np.mean(train_loss)}] Val Score : [{score}])')\n",
        "\n",
        "#       if self.scheduler is not None:\n",
        "#         self.scheduler.step(score)\n",
        "\n",
        "#       if best_score < score:\n",
        "#           best_score = score\n",
        "#           torch.save(model.module.state_dict(), './best_model.pth', _use_new_zipfile_serialization=False)\n",
        "\n",
        "#     def validation(self, eval_model, thr):\n",
        "#         cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "#         eval_model.eval()\n",
        "#         pred = []\n",
        "#         true = []\n",
        "#         with torch.no_grad():\n",
        "#             for x, y in iter(self.val_loader):\n",
        "#                 x = x.float().to(self.device)\n",
        "\n",
        "#                 _x = self.model(x)\n",
        "#                 diff = cos(x, _x).cpu().tolist()\n",
        "#                 batch_pred = np.where(np.array(diff)<thr, 1,0).tolist()\n",
        "#                 pred += batch_pred\n",
        "#                 true += y.tolist()\n",
        "\n",
        "#         return f1_score(true, pred, average='macro')     "
      ],
      "metadata": {
        "id": "oKazsDY-NkZn"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer():\n",
        "    def __init__(self, model, optimizer, train_loader, val_loader, scheduler, device):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.scheduler = scheduler\n",
        "        self.device = device\n",
        "        # Loss Function\n",
        "        self.criterion = nn.L1Loss().to(self.device)\n",
        "        \n",
        "    def fit(self, ):\n",
        "        self.model.to(self.device)\n",
        "        best_score = 0\n",
        "        for epoch in range(EPOCHS):\n",
        "            self.model.train()\n",
        "            train_loss = []\n",
        "            for x in iter(self.train_loader):\n",
        "                x = x.float().to(self.device)\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                _x = self.model(x)\n",
        "                loss = self.criterion(x, _x)\n",
        "\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                train_loss.append(loss.item())\n",
        "\n",
        "            score = self.validation(self.model, 0.95)\n",
        "            print(f'Epoch : [{epoch}] Train loss : [{np.mean(train_loss)}] Val Score : [{score}])')\n",
        "\n",
        "            if self.scheduler is not None:\n",
        "                self.scheduler.step(score)\n",
        "\n",
        "            if best_score < score:\n",
        "                best_score = score\n",
        "                torch.save(model.module.state_dict(), './best_model.pth', _use_new_zipfile_serialization=False)\n",
        "    \n",
        "    def validation(self, eval_model, thr):\n",
        "        cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "        eval_model.eval()\n",
        "        pred = []\n",
        "        true = []\n",
        "        with torch.no_grad():\n",
        "            for x, y in iter(self.val_loader):\n",
        "                x = x.float().to(self.device)\n",
        "\n",
        "                _x = self.model(x)\n",
        "                diff = cos(x, _x).cpu().tolist()\n",
        "                batch_pred = np.where(np.array(diff)<thr, 1,0).tolist()\n",
        "                pred += batch_pred\n",
        "                true += y.tolist()\n",
        "\n",
        "        return f1_score(true, pred, average='macro')"
      ],
      "metadata": {
        "id": "jhtCzRVzSMbd"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.DataParallel(AutoEncoder())"
      ],
      "metadata": {
        "id": "6rIcHRUyRLgg"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qL86w42sRWKv",
        "outputId": "62974402-b1a5-4980-f192-fbc4a6e9a212"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataParallel(\n",
              "  (module): AutoEncoder(\n",
              "    (Encoder): Sequential(\n",
              "      (0): Linear(in_features=30, out_features=64, bias=True)\n",
              "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): LeakyReLU(negative_slope=0.01)\n",
              "      (3): Linear(in_features=64, out_features=128, bias=True)\n",
              "      (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): LeakyReLU(negative_slope=0.01)\n",
              "    )\n",
              "    (Decoder): Sequential(\n",
              "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
              "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): LeakyReLU(negative_slope=0.01)\n",
              "      (3): Linear(in_features=64, out_features=30, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=LR)"
      ],
      "metadata": {
        "id": "gEESTUknRid3"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10, threshold_mode='abs', min_lr=1e-8, verbose=True)\n"
      ],
      "metadata": {
        "id": "Nkb9RaIaRufX"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(model, optimizer, train_loader, val_loader, scheduler, device)"
      ],
      "metadata": {
        "id": "1juAuMaMSGtv"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.fit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FifS4fVQSIqO",
        "outputId": "c4dcd4bd-6211-4e41-aae6-aa636fd17360"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : [0] Train loss : [0.5440883040428162] Val Score : [0.0010881344945152598])\n",
            "Epoch : [1] Train loss : [0.36390640480177744] Val Score : [0.060320716220602234])\n",
            "Epoch : [2] Train loss : [0.2796274168150766] Val Score : [0.23540084729198593])\n",
            "Epoch : [3] Train loss : [0.22501157649925776] Val Score : [0.38806282948751186])\n",
            "Epoch : [4] Train loss : [0.1907194490943636] Val Score : [0.4471790184357988])\n",
            "Epoch : [5] Train loss : [0.1690957588808877] Val Score : [0.47231352501927615])\n",
            "Epoch : [6] Train loss : [0.15324431657791138] Val Score : [0.48395057940305924])\n",
            "Epoch : [7] Train loss : [0.1420836363519941] Val Score : [0.49158783071438084])\n",
            "Epoch : [8] Train loss : [0.1338188030890056] Val Score : [0.49542788338492944])\n",
            "Epoch : [9] Train loss : [0.12757716647216252] Val Score : [0.49940916819306996])\n",
            "Epoch : [10] Train loss : [0.12314313756568092] Val Score : [0.5009380973388867])\n",
            "Epoch : [11] Train loss : [0.11723028761999947] Val Score : [0.5027624223941444])\n",
            "Epoch : [12] Train loss : [0.11173857216324125] Val Score : [0.5046863216903836])\n",
            "Epoch : [13] Train loss : [0.10846255719661713] Val Score : [0.5050702088229284])\n",
            "Epoch : [14] Train loss : [0.10719364987952369] Val Score : [0.5058987795198697])\n",
            "Epoch : [15] Train loss : [0.1027413500206811] Val Score : [0.5066624145728117])\n",
            "Epoch : [16] Train loss : [0.10008585453033447] Val Score : [0.5076846985307293])\n",
            "Epoch : [17] Train loss : [0.09829079679080419] Val Score : [0.510303376299578])\n",
            "Epoch : [18] Train loss : [0.09330592091594424] Val Score : [0.51531006027499])\n",
            "Epoch : [19] Train loss : [0.0906141187463488] Val Score : [0.5260565911276457])\n",
            "Epoch : [20] Train loss : [0.08825443578617913] Val Score : [0.5306646863632829])\n",
            "Epoch : [21] Train loss : [0.08876241530690875] Val Score : [0.5301750712820299])\n",
            "Epoch : [22] Train loss : [0.0842887886932918] Val Score : [0.5312272431088949])\n",
            "Epoch : [23] Train loss : [0.08293161221912929] Val Score : [0.5318027135980778])\n",
            "Epoch : [24] Train loss : [0.08376790370259966] Val Score : [0.5331987221566994])\n",
            "Epoch : [25] Train loss : [0.08160105028322764] Val Score : [0.5342447543786599])\n",
            "Epoch : [26] Train loss : [0.07957477335418973] Val Score : [0.5361642339654805])\n",
            "Epoch : [27] Train loss : [0.07637461381299156] Val Score : [0.5402075057997111])\n",
            "Epoch : [28] Train loss : [0.07775275515658515] Val Score : [0.5398539540351446])\n",
            "Epoch : [29] Train loss : [0.07731983278478895] Val Score : [0.5403859911539594])\n",
            "Epoch : [30] Train loss : [0.0742077146257673] Val Score : [0.54204592959581])\n",
            "Epoch : [31] Train loss : [0.07171548051493508] Val Score : [0.5444220066101826])\n",
            "Epoch : [32] Train loss : [0.07124859413930348] Val Score : [0.5455797786442947])\n",
            "Epoch : [33] Train loss : [0.06910334208181926] Val Score : [0.54536585119794])\n",
            "Epoch : [34] Train loss : [0.06741481806550707] Val Score : [0.5447332515688151])\n",
            "Epoch : [35] Train loss : [0.0681981018611363] Val Score : [0.5488671189923648])\n",
            "Epoch : [36] Train loss : [0.06634360019649778] Val Score : [0.5468967903812862])\n",
            "Epoch : [37] Train loss : [0.06698225757905416] Val Score : [0.5464512870526415])\n",
            "Epoch : [38] Train loss : [0.06825604396206993] Val Score : [0.5474631286456717])\n",
            "Epoch : [39] Train loss : [0.06392205879092216] Val Score : [0.5488671189923648])\n",
            "Epoch : [40] Train loss : [0.06385487424475807] Val Score : [0.5493497907415081])\n",
            "Epoch : [41] Train loss : [0.06539162886994225] Val Score : [0.5494716466579079])\n",
            "Epoch : [42] Train loss : [0.06486557317631585] Val Score : [0.5493497907415081])\n",
            "Epoch : [43] Train loss : [0.06621465725558144] Val Score : [0.5508445183715132])\n",
            "Epoch : [44] Train loss : [0.062347104506833215] Val Score : [0.5512296049680054])\n",
            "Epoch : [45] Train loss : [0.061766408916030614] Val Score : [0.5502129979112463])\n",
            "Epoch : [46] Train loss : [0.06092996150255203] Val Score : [0.5502129979112463])\n",
            "Epoch : [47] Train loss : [0.06290261553866523] Val Score : [0.5508445183715132])\n",
            "Epoch : [48] Train loss : [0.0610620491206646] Val Score : [0.5539227946765344])\n",
            "Epoch : [49] Train loss : [0.06177284408892904] Val Score : [0.5535042161494685])\n",
            "Epoch : [50] Train loss : [0.06344029200928551] Val Score : [0.5540635212673894])\n",
            "Epoch : [51] Train loss : [0.05855614851628031] Val Score : [0.5544893606698819])\n",
            "Epoch : [52] Train loss : [0.05659105841602598] Val Score : [0.5543468000359598])\n",
            "Epoch : [53] Train loss : [0.05757072993687221] Val Score : [0.555800828312292])\n",
            "Epoch : [54] Train loss : [0.05938430343355451] Val Score : [0.5567046530896047])\n",
            "Epoch : [55] Train loss : [0.058032583977494924] Val Score : [0.5567046530896047])\n",
            "Epoch : [56] Train loss : [0.05451559967228344] Val Score : [0.5582667196871773])\n",
            "Epoch : [57] Train loss : [0.054989748767444065] Val Score : [0.5582667196871773])\n",
            "Epoch : [58] Train loss : [0.0583705407168184] Val Score : [0.5616191340522515])\n",
            "Epoch : [59] Train loss : [0.05701843276619911] Val Score : [0.563606854805332])\n",
            "Epoch : [60] Train loss : [0.05639302836997168] Val Score : [0.5679345323877522])\n",
            "Epoch : [61] Train loss : [0.0559001767209598] Val Score : [0.5683535623147975])\n",
            "Epoch : [62] Train loss : [0.05318619789821761] Val Score : [0.5752370096002138])\n",
            "Epoch : [63] Train loss : [0.05487242713570595] Val Score : [0.5866045319852622])\n",
            "Epoch : [64] Train loss : [0.05276120986257281] Val Score : [0.5958386457083363])\n",
            "Epoch : [65] Train loss : [0.053478574114186425] Val Score : [0.6346887404024459])\n",
            "Epoch : [66] Train loss : [0.05146959690111024] Val Score : [0.7168192118976862])\n",
            "Epoch : [67] Train loss : [0.05108523794582912] Val Score : [0.7655703273293624])\n",
            "Epoch : [68] Train loss : [0.0521676130592823] Val Score : [0.7713696202996474])\n",
            "Epoch : [69] Train loss : [0.05137682280370167] Val Score : [0.7805557779616334])\n",
            "Epoch : [70] Train loss : [0.050764787409986765] Val Score : [0.7870308296420961])\n",
            "Epoch : [71] Train loss : [0.05076510352747781] Val Score : [0.777425875747303])\n",
            "Epoch : [72] Train loss : [0.050698530461106985] Val Score : [0.7870308296420961])\n",
            "Epoch : [73] Train loss : [0.05028442133750234] Val Score : [0.79380975986869])\n",
            "Epoch : [74] Train loss : [0.04829164647630283] Val Score : [0.7973199624677456])\n",
            "Epoch : [75] Train loss : [0.0469090464924063] Val Score : [0.7903809848799157])\n",
            "Epoch : [76] Train loss : [0.04780347113098417] Val Score : [0.8009145358549308])\n",
            "Epoch : [77] Train loss : [0.05021888922367777] Val Score : [0.8244378451249526])\n",
            "Epoch : [78] Train loss : [0.053256280188049586] Val Score : [0.8202665410912253])\n",
            "Epoch : [79] Train loss : [0.049313812383583615] Val Score : [0.8244378451249526])\n",
            "Epoch : [80] Train loss : [0.05063405100788389] Val Score : [0.8287186884323108])\n",
            "Epoch : [81] Train loss : [0.048076646136386056] Val Score : [0.8470287373843977])\n",
            "Epoch : [82] Train loss : [0.046441239437886646] Val Score : [0.8519279892324237])\n",
            "Epoch : [83] Train loss : [0.04674692239080157] Val Score : [0.8519279892324237])\n",
            "Epoch : [84] Train loss : [0.04757855140737125] Val Score : [0.8621517488551477])\n",
            "Epoch : [85] Train loss : [0.0492419068302427] Val Score : [0.890501890608512])\n",
            "Epoch : [86] Train loss : [0.04918888636997768] Val Score : [0.8786471773914175])\n",
            "Epoch : [87] Train loss : [0.04480405205062458] Val Score : [0.8967110829723166])\n",
            "Epoch : [88] Train loss : [0.041906721357788355] Val Score : [0.8967110829723166])\n",
            "Epoch : [89] Train loss : [0.043488837246383936] Val Score : [0.9097393418694286])\n",
            "Epoch : [90] Train loss : [0.042885674961975644] Val Score : [0.9097393418694286])\n",
            "Epoch : [91] Train loss : [0.04369834331529481] Val Score : [0.9165787375726882])\n",
            "Epoch : [92] Train loss : [0.04557633399963379] Val Score : [0.9165787375726882])\n",
            "Epoch : [93] Train loss : [0.045851051807403564] Val Score : [0.9165787375726882])\n",
            "Epoch : [94] Train loss : [0.045221185577767234] Val Score : [0.9165787375726882])\n",
            "Epoch : [95] Train loss : [0.04781049170664379] Val Score : [0.9165787375726882])\n",
            "Epoch : [96] Train loss : [0.04391158691474369] Val Score : [0.9165787375726882])\n",
            "Epoch : [97] Train loss : [0.04304036551288196] Val Score : [0.9165787375726882])\n",
            "Epoch : [98] Train loss : [0.043820929314408986] Val Score : [0.9097393418694286])\n",
            "Epoch : [99] Train loss : [0.04372212024671691] Val Score : [0.9165787375726882])\n",
            "Epoch : [100] Train loss : [0.04322961185659681] Val Score : [0.9165787375726882])\n",
            "Epoch : [101] Train loss : [0.04593515183244433] Val Score : [0.9165787375726882])\n",
            "Epoch : [102] Train loss : [0.04251130138124738] Val Score : [0.9097393418694286])\n",
            "Epoch 00103: reducing learning rate of group 0 to 5.0000e-03.\n",
            "Epoch : [103] Train loss : [0.03758699500135013] Val Score : [0.9165787375726882])\n",
            "Epoch : [104] Train loss : [0.035064897899116786] Val Score : [0.9165787375726882])\n",
            "Epoch : [105] Train loss : [0.03568915756685393] Val Score : [0.9165787375726882])\n",
            "Epoch : [106] Train loss : [0.0349200147071055] Val Score : [0.9165787375726882])\n",
            "Epoch : [107] Train loss : [0.03343059016125543] Val Score : [0.9165787375726882])\n",
            "Epoch : [108] Train loss : [0.037127270230225155] Val Score : [0.9165787375726882])\n",
            "Epoch : [109] Train loss : [0.03530764899083546] Val Score : [0.9165787375726882])\n",
            "Epoch : [110] Train loss : [0.03609902198825564] Val Score : [0.9165787375726882])\n",
            "Epoch : [111] Train loss : [0.036749748779194694] Val Score : [0.9165787375726882])\n",
            "Epoch : [112] Train loss : [0.034669729215758185] Val Score : [0.9165787375726882])\n",
            "Epoch : [113] Train loss : [0.03464120892541749] Val Score : [0.9165787375726882])\n",
            "Epoch 00114: reducing learning rate of group 0 to 2.5000e-03.\n",
            "Epoch : [114] Train loss : [0.032284443133643696] Val Score : [0.9165787375726882])\n",
            "Epoch : [115] Train loss : [0.02841448171862534] Val Score : [0.9165787375726882])\n",
            "Epoch : [116] Train loss : [0.032021029985376766] Val Score : [0.9165787375726882])\n",
            "Epoch : [117] Train loss : [0.028667534568480084] Val Score : [0.9165787375726882])\n",
            "Epoch : [118] Train loss : [0.03173934668302536] Val Score : [0.9165787375726882])\n",
            "Epoch : [119] Train loss : [0.02997939528099128] Val Score : [0.9165787375726882])\n",
            "Epoch : [120] Train loss : [0.028119158798030446] Val Score : [0.9165787375726882])\n",
            "Epoch : [121] Train loss : [0.02888506917016847] Val Score : [0.9165787375726882])\n",
            "Epoch : [122] Train loss : [0.028311568179300854] Val Score : [0.9165787375726882])\n",
            "Epoch : [123] Train loss : [0.03019482615802969] Val Score : [0.9165787375726882])\n",
            "Epoch : [124] Train loss : [0.028716823618326868] Val Score : [0.9165787375726882])\n",
            "Epoch 00125: reducing learning rate of group 0 to 1.2500e-03.\n",
            "Epoch : [125] Train loss : [0.02743968873151711] Val Score : [0.9165787375726882])\n",
            "Epoch : [126] Train loss : [0.025482187579785074] Val Score : [0.9165787375726882])\n",
            "Epoch : [127] Train loss : [0.025740564933844974] Val Score : [0.9165787375726882])\n",
            "Epoch : [128] Train loss : [0.025116018152662685] Val Score : [0.9165787375726882])\n",
            "Epoch : [129] Train loss : [0.025045503729156086] Val Score : [0.9165787375726882])\n",
            "Epoch : [130] Train loss : [0.026844872693930353] Val Score : [0.9165787375726882])\n",
            "Epoch : [131] Train loss : [0.026523019852382795] Val Score : [0.9165787375726882])\n",
            "Epoch : [132] Train loss : [0.02591767135475363] Val Score : [0.9165787375726882])\n",
            "Epoch : [133] Train loss : [0.026289456923093115] Val Score : [0.9165787375726882])\n",
            "Epoch : [134] Train loss : [0.026470343449286053] Val Score : [0.9165787375726882])\n",
            "Epoch : [135] Train loss : [0.0265511902315276] Val Score : [0.9165787375726882])\n",
            "Epoch 00136: reducing learning rate of group 0 to 6.2500e-04.\n",
            "Epoch : [136] Train loss : [0.025190415925213268] Val Score : [0.9165787375726882])\n",
            "Epoch : [137] Train loss : [0.02371327579021454] Val Score : [0.9165787375726882])\n",
            "Epoch : [138] Train loss : [0.025268157944083214] Val Score : [0.9165787375726882])\n",
            "Epoch : [139] Train loss : [0.023466143757104874] Val Score : [0.9165787375726882])\n",
            "Epoch : [140] Train loss : [0.024242849222251346] Val Score : [0.9165787375726882])\n",
            "Epoch : [141] Train loss : [0.025775852746197155] Val Score : [0.9165787375726882])\n",
            "Epoch : [142] Train loss : [0.025332504883408546] Val Score : [0.9165787375726882])\n",
            "Epoch : [143] Train loss : [0.02433756978384086] Val Score : [0.9165787375726882])\n",
            "Epoch : [144] Train loss : [0.025118139439395497] Val Score : [0.9165787375726882])\n",
            "Epoch : [145] Train loss : [0.024251829566700116] Val Score : [0.9165787375726882])\n",
            "Epoch : [146] Train loss : [0.023450186210019246] Val Score : [0.9165787375726882])\n",
            "Epoch 00147: reducing learning rate of group 0 to 3.1250e-04.\n",
            "Epoch : [147] Train loss : [0.024017072947961942] Val Score : [0.9165787375726882])\n",
            "Epoch : [148] Train loss : [0.02265761872487409] Val Score : [0.9165787375726882])\n",
            "Epoch : [149] Train loss : [0.022282031763877188] Val Score : [0.9165787375726882])\n",
            "Epoch : [150] Train loss : [0.023247421852179935] Val Score : [0.9165787375726882])\n",
            "Epoch : [151] Train loss : [0.023248455354145596] Val Score : [0.9165787375726882])\n",
            "Epoch : [152] Train loss : [0.022368130939347402] Val Score : [0.9165787375726882])\n",
            "Epoch : [153] Train loss : [0.024593256946120943] Val Score : [0.9165787375726882])\n",
            "Epoch : [154] Train loss : [0.022759923711419106] Val Score : [0.9165787375726882])\n",
            "Epoch : [155] Train loss : [0.023275301924773624] Val Score : [0.9165787375726882])\n",
            "Epoch : [156] Train loss : [0.02331015786954335] Val Score : [0.9165787375726882])\n",
            "Epoch : [157] Train loss : [0.022797643872244016] Val Score : [0.9165787375726882])\n",
            "Epoch 00158: reducing learning rate of group 0 to 1.5625e-04.\n",
            "Epoch : [158] Train loss : [0.022639048951012746] Val Score : [0.9165787375726882])\n",
            "Epoch : [159] Train loss : [0.02322739815073354] Val Score : [0.9165787375726882])\n",
            "Epoch : [160] Train loss : [0.02088089074407305] Val Score : [0.9165787375726882])\n",
            "Epoch : [161] Train loss : [0.022407020309141705] Val Score : [0.9165787375726882])\n",
            "Epoch : [162] Train loss : [0.021856337785720825] Val Score : [0.9165787375726882])\n",
            "Epoch : [163] Train loss : [0.025791396253875325] Val Score : [0.9165787375726882])\n",
            "Epoch : [164] Train loss : [0.020559251308441162] Val Score : [0.9165787375726882])\n",
            "Epoch : [165] Train loss : [0.024150574845927104] Val Score : [0.9165787375726882])\n",
            "Epoch : [166] Train loss : [0.021701318078807423] Val Score : [0.9165787375726882])\n",
            "Epoch : [167] Train loss : [0.022848656933222498] Val Score : [0.9165787375726882])\n",
            "Epoch : [168] Train loss : [0.02163866880748953] Val Score : [0.9165787375726882])\n",
            "Epoch 00169: reducing learning rate of group 0 to 7.8125e-05.\n",
            "Epoch : [169] Train loss : [0.025322668786559786] Val Score : [0.9165787375726882])\n",
            "Epoch : [170] Train loss : [0.02290947682091168] Val Score : [0.9165787375726882])\n",
            "Epoch : [171] Train loss : [0.023262899102909223] Val Score : [0.9165787375726882])\n",
            "Epoch : [172] Train loss : [0.02151142619550228] Val Score : [0.9165787375726882])\n",
            "Epoch : [173] Train loss : [0.02258689222591264] Val Score : [0.9165787375726882])\n",
            "Epoch : [174] Train loss : [0.021863574694309915] Val Score : [0.9165787375726882])\n",
            "Epoch : [175] Train loss : [0.022779471640075957] Val Score : [0.9165787375726882])\n",
            "Epoch : [176] Train loss : [0.023374213437948908] Val Score : [0.9165787375726882])\n",
            "Epoch : [177] Train loss : [0.02174662080194269] Val Score : [0.9165787375726882])\n",
            "Epoch : [178] Train loss : [0.022491943889430592] Val Score : [0.9165787375726882])\n",
            "Epoch : [179] Train loss : [0.021227158339960233] Val Score : [0.9165787375726882])\n",
            "Epoch 00180: reducing learning rate of group 0 to 3.9063e-05.\n",
            "Epoch : [180] Train loss : [0.021438395338399068] Val Score : [0.9165787375726882])\n",
            "Epoch : [181] Train loss : [0.021071518638304303] Val Score : [0.9165787375726882])\n",
            "Epoch : [182] Train loss : [0.0221421622804233] Val Score : [0.9165787375726882])\n",
            "Epoch : [183] Train loss : [0.0228653329291514] Val Score : [0.9165787375726882])\n",
            "Epoch : [184] Train loss : [0.022370953911117146] Val Score : [0.9165787375726882])\n",
            "Epoch : [185] Train loss : [0.022094479895063808] Val Score : [0.9165787375726882])\n",
            "Epoch : [186] Train loss : [0.02207084691950253] Val Score : [0.9165787375726882])\n",
            "Epoch : [187] Train loss : [0.02178885228931904] Val Score : [0.9165787375726882])\n",
            "Epoch : [188] Train loss : [0.022840144644890512] Val Score : [0.9165787375726882])\n",
            "Epoch : [189] Train loss : [0.023229175912482396] Val Score : [0.9165787375726882])\n",
            "Epoch : [190] Train loss : [0.02348229581756251] Val Score : [0.9165787375726882])\n",
            "Epoch 00191: reducing learning rate of group 0 to 1.9531e-05.\n",
            "Epoch : [191] Train loss : [0.02406294271349907] Val Score : [0.9165787375726882])\n",
            "Epoch : [192] Train loss : [0.022540925070643425] Val Score : [0.9165787375726882])\n",
            "Epoch : [193] Train loss : [0.022503480847392763] Val Score : [0.9165787375726882])\n",
            "Epoch : [194] Train loss : [0.02325862007481711] Val Score : [0.9165787375726882])\n",
            "Epoch : [195] Train loss : [0.023344230705073903] Val Score : [0.9165787375726882])\n",
            "Epoch : [196] Train loss : [0.02277014989938055] Val Score : [0.9165787375726882])\n",
            "Epoch : [197] Train loss : [0.02157100130404745] Val Score : [0.9165787375726882])\n",
            "Epoch : [198] Train loss : [0.02248059372816767] Val Score : [0.9165787375726882])\n",
            "Epoch : [199] Train loss : [0.023158116000039235] Val Score : [0.9165787375726882])\n",
            "Epoch : [200] Train loss : [0.02258252991097314] Val Score : [0.9165787375726882])\n",
            "Epoch : [201] Train loss : [0.023713645924414908] Val Score : [0.9165787375726882])\n",
            "Epoch 00202: reducing learning rate of group 0 to 9.7656e-06.\n",
            "Epoch : [202] Train loss : [0.022122062210525786] Val Score : [0.9165787375726882])\n",
            "Epoch : [203] Train loss : [0.022629186511039734] Val Score : [0.9165787375726882])\n",
            "Epoch : [204] Train loss : [0.02184354779975755] Val Score : [0.9165787375726882])\n",
            "Epoch : [205] Train loss : [0.020416557788848877] Val Score : [0.9165787375726882])\n",
            "Epoch : [206] Train loss : [0.022363917902112007] Val Score : [0.9165787375726882])\n",
            "Epoch : [207] Train loss : [0.024106195462601527] Val Score : [0.9165787375726882])\n",
            "Epoch : [208] Train loss : [0.021129329555800984] Val Score : [0.9165787375726882])\n",
            "Epoch : [209] Train loss : [0.022984698148710386] Val Score : [0.9165787375726882])\n",
            "Epoch : [210] Train loss : [0.025305179080792835] Val Score : [0.9165787375726882])\n",
            "Epoch : [211] Train loss : [0.021690298404012407] Val Score : [0.9165787375726882])\n",
            "Epoch : [212] Train loss : [0.021253429619329318] Val Score : [0.9165787375726882])\n",
            "Epoch 00213: reducing learning rate of group 0 to 4.8828e-06.\n",
            "Epoch : [213] Train loss : [0.02435398367898805] Val Score : [0.9165787375726882])\n",
            "Epoch : [214] Train loss : [0.022019190979855403] Val Score : [0.9165787375726882])\n",
            "Epoch : [215] Train loss : [0.02207538591963904] Val Score : [0.9165787375726882])\n",
            "Epoch : [216] Train loss : [0.020608345312731608] Val Score : [0.9165787375726882])\n",
            "Epoch : [217] Train loss : [0.02313547847526414] Val Score : [0.9165787375726882])\n",
            "Epoch : [218] Train loss : [0.02136373147368431] Val Score : [0.9165787375726882])\n",
            "Epoch : [219] Train loss : [0.02274788516972746] Val Score : [0.9165787375726882])\n",
            "Epoch : [220] Train loss : [0.022943108209541867] Val Score : [0.9165787375726882])\n",
            "Epoch : [221] Train loss : [0.022656129407031194] Val Score : [0.9165787375726882])\n",
            "Epoch : [222] Train loss : [0.021939771249890327] Val Score : [0.9165787375726882])\n",
            "Epoch : [223] Train loss : [0.022895426090274538] Val Score : [0.9165787375726882])\n",
            "Epoch 00224: reducing learning rate of group 0 to 2.4414e-06.\n",
            "Epoch : [224] Train loss : [0.024137009733489583] Val Score : [0.9165787375726882])\n",
            "Epoch : [225] Train loss : [0.022141315575156893] Val Score : [0.9165787375726882])\n",
            "Epoch : [226] Train loss : [0.021405673186693872] Val Score : [0.9165787375726882])\n",
            "Epoch : [227] Train loss : [0.021853051547493254] Val Score : [0.9165787375726882])\n",
            "Epoch : [228] Train loss : [0.02323954046836921] Val Score : [0.9165787375726882])\n",
            "Epoch : [229] Train loss : [0.02191042314682688] Val Score : [0.9165787375726882])\n",
            "Epoch : [230] Train loss : [0.02279931679368019] Val Score : [0.9165787375726882])\n",
            "Epoch : [231] Train loss : [0.020891488130603517] Val Score : [0.9165787375726882])\n",
            "Epoch : [232] Train loss : [0.022491772526076863] Val Score : [0.9165787375726882])\n",
            "Epoch : [233] Train loss : [0.02311235586447375] Val Score : [0.9165787375726882])\n",
            "Epoch : [234] Train loss : [0.022663300590855733] Val Score : [0.9165787375726882])\n",
            "Epoch 00235: reducing learning rate of group 0 to 1.2207e-06.\n",
            "Epoch : [235] Train loss : [0.02222344864692007] Val Score : [0.9165787375726882])\n",
            "Epoch : [236] Train loss : [0.022377589451415197] Val Score : [0.9165787375726882])\n",
            "Epoch : [237] Train loss : [0.02316930757037231] Val Score : [0.9165787375726882])\n",
            "Epoch : [238] Train loss : [0.022421278059482574] Val Score : [0.9165787375726882])\n",
            "Epoch : [239] Train loss : [0.021440894476005008] Val Score : [0.9165787375726882])\n",
            "Epoch : [240] Train loss : [0.022973358098949705] Val Score : [0.9165787375726882])\n",
            "Epoch : [241] Train loss : [0.02227224303143365] Val Score : [0.9165787375726882])\n",
            "Epoch : [242] Train loss : [0.024082767644098828] Val Score : [0.9165787375726882])\n",
            "Epoch : [243] Train loss : [0.0218841481421675] Val Score : [0.9165787375726882])\n",
            "Epoch : [244] Train loss : [0.022039534524083138] Val Score : [0.9165787375726882])\n",
            "Epoch : [245] Train loss : [0.021870000287890434] Val Score : [0.9165787375726882])\n",
            "Epoch 00246: reducing learning rate of group 0 to 6.1035e-07.\n",
            "Epoch : [246] Train loss : [0.021752811702234403] Val Score : [0.9165787375726882])\n",
            "Epoch : [247] Train loss : [0.02130629601223128] Val Score : [0.9165787375726882])\n",
            "Epoch : [248] Train loss : [0.023146914850388254] Val Score : [0.9165787375726882])\n",
            "Epoch : [249] Train loss : [0.02125012768166406] Val Score : [0.9165787375726882])\n",
            "Epoch : [250] Train loss : [0.02231546863913536] Val Score : [0.9165787375726882])\n",
            "Epoch : [251] Train loss : [0.02205987966486386] Val Score : [0.9165787375726882])\n",
            "Epoch : [252] Train loss : [0.022894821794969693] Val Score : [0.9165787375726882])\n",
            "Epoch : [253] Train loss : [0.022624369710683823] Val Score : [0.9165787375726882])\n",
            "Epoch : [254] Train loss : [0.022404095690165247] Val Score : [0.9165787375726882])\n",
            "Epoch : [255] Train loss : [0.022050647863319943] Val Score : [0.9165787375726882])\n",
            "Epoch : [256] Train loss : [0.022256687017423765] Val Score : [0.9165787375726882])\n",
            "Epoch 00257: reducing learning rate of group 0 to 3.0518e-07.\n",
            "Epoch : [257] Train loss : [0.022450544206159457] Val Score : [0.9165787375726882])\n",
            "Epoch : [258] Train loss : [0.02210792393556663] Val Score : [0.9165787375726882])\n",
            "Epoch : [259] Train loss : [0.02186632262808936] Val Score : [0.9165787375726882])\n",
            "Epoch : [260] Train loss : [0.021038931661418507] Val Score : [0.9165787375726882])\n",
            "Epoch : [261] Train loss : [0.02324338204094342] Val Score : [0.9165787375726882])\n",
            "Epoch : [262] Train loss : [0.021008729934692383] Val Score : [0.9165787375726882])\n",
            "Epoch : [263] Train loss : [0.022906965176974024] Val Score : [0.9165787375726882])\n",
            "Epoch : [264] Train loss : [0.022498366023812975] Val Score : [0.9165787375726882])\n",
            "Epoch : [265] Train loss : [0.024168751069477627] Val Score : [0.9165787375726882])\n",
            "Epoch : [266] Train loss : [0.02095453760453633] Val Score : [0.9165787375726882])\n",
            "Epoch : [267] Train loss : [0.021883784660271237] Val Score : [0.9165787375726882])\n",
            "Epoch 00268: reducing learning rate of group 0 to 1.5259e-07.\n",
            "Epoch : [268] Train loss : [0.023416003212332726] Val Score : [0.9165787375726882])\n",
            "Epoch : [269] Train loss : [0.021190974063106945] Val Score : [0.9165787375726882])\n",
            "Epoch : [270] Train loss : [0.02084647598011153] Val Score : [0.9165787375726882])\n",
            "Epoch : [271] Train loss : [0.022122185677289963] Val Score : [0.9165787375726882])\n",
            "Epoch : [272] Train loss : [0.021600847531642233] Val Score : [0.9165787375726882])\n",
            "Epoch : [273] Train loss : [0.022191443613597324] Val Score : [0.9165787375726882])\n",
            "Epoch : [274] Train loss : [0.02168440499476024] Val Score : [0.9165787375726882])\n",
            "Epoch : [275] Train loss : [0.022162397791232382] Val Score : [0.9165787375726882])\n",
            "Epoch : [276] Train loss : [0.024612403341702054] Val Score : [0.9165787375726882])\n",
            "Epoch : [277] Train loss : [0.024408326617309024] Val Score : [0.9165787375726882])\n",
            "Epoch : [278] Train loss : [0.021434590752635683] Val Score : [0.9165787375726882])\n",
            "Epoch 00279: reducing learning rate of group 0 to 7.6294e-08.\n",
            "Epoch : [279] Train loss : [0.02221085530306612] Val Score : [0.9165787375726882])\n",
            "Epoch : [280] Train loss : [0.02282394841313362] Val Score : [0.9165787375726882])\n",
            "Epoch : [281] Train loss : [0.02137058787047863] Val Score : [0.9165787375726882])\n",
            "Epoch : [282] Train loss : [0.0216957830957004] Val Score : [0.9165787375726882])\n",
            "Epoch : [283] Train loss : [0.021784178912639618] Val Score : [0.9165787375726882])\n",
            "Epoch : [284] Train loss : [0.023072685514177595] Val Score : [0.9165787375726882])\n",
            "Epoch : [285] Train loss : [0.0225094450371606] Val Score : [0.9165787375726882])\n",
            "Epoch : [286] Train loss : [0.02306943679494517] Val Score : [0.9165787375726882])\n",
            "Epoch : [287] Train loss : [0.021589876019528935] Val Score : [0.9165787375726882])\n",
            "Epoch : [288] Train loss : [0.02331527801496642] Val Score : [0.9165787375726882])\n",
            "Epoch : [289] Train loss : [0.02118232979306153] Val Score : [0.9165787375726882])\n",
            "Epoch 00290: reducing learning rate of group 0 to 3.8147e-08.\n",
            "Epoch : [290] Train loss : [0.02214575745165348] Val Score : [0.9165787375726882])\n",
            "Epoch : [291] Train loss : [0.02234150974878243] Val Score : [0.9165787375726882])\n",
            "Epoch : [292] Train loss : [0.02158633725983756] Val Score : [0.9165787375726882])\n",
            "Epoch : [293] Train loss : [0.02211917138525418] Val Score : [0.9165787375726882])\n",
            "Epoch : [294] Train loss : [0.023560952661292895] Val Score : [0.9165787375726882])\n",
            "Epoch : [295] Train loss : [0.023966736027172635] Val Score : [0.9165787375726882])\n",
            "Epoch : [296] Train loss : [0.022404605522751808] Val Score : [0.9165787375726882])\n",
            "Epoch : [297] Train loss : [0.02251387094812734] Val Score : [0.9165787375726882])\n",
            "Epoch : [298] Train loss : [0.021796074296746935] Val Score : [0.9165787375726882])\n",
            "Epoch : [299] Train loss : [0.02433870466692107] Val Score : [0.9165787375726882])\n",
            "Epoch : [300] Train loss : [0.022080418254647936] Val Score : [0.9165787375726882])\n",
            "Epoch 00301: reducing learning rate of group 0 to 1.9073e-08.\n",
            "Epoch : [301] Train loss : [0.022968208949480737] Val Score : [0.9165787375726882])\n",
            "Epoch : [302] Train loss : [0.02052490971982479] Val Score : [0.9165787375726882])\n",
            "Epoch : [303] Train loss : [0.022672447242907116] Val Score : [0.9165787375726882])\n",
            "Epoch : [304] Train loss : [0.021807455058608736] Val Score : [0.9165787375726882])\n",
            "Epoch : [305] Train loss : [0.023606727964111736] Val Score : [0.9165787375726882])\n",
            "Epoch : [306] Train loss : [0.022123347967863083] Val Score : [0.9165787375726882])\n",
            "Epoch : [307] Train loss : [0.023735144308635166] Val Score : [0.9165787375726882])\n",
            "Epoch : [308] Train loss : [0.021953761045421873] Val Score : [0.9165787375726882])\n",
            "Epoch : [309] Train loss : [0.022386420784252032] Val Score : [0.9165787375726882])\n",
            "Epoch : [310] Train loss : [0.022706286449517523] Val Score : [0.9165787375726882])\n",
            "Epoch : [311] Train loss : [0.022631384964500154] Val Score : [0.9165787375726882])\n",
            "Epoch : [312] Train loss : [0.02118906272309167] Val Score : [0.9165787375726882])\n",
            "Epoch : [313] Train loss : [0.022589666236724173] Val Score : [0.9165787375726882])\n",
            "Epoch : [314] Train loss : [0.021711273651037897] Val Score : [0.9165787375726882])\n",
            "Epoch : [315] Train loss : [0.023516215650098666] Val Score : [0.9165787375726882])\n",
            "Epoch : [316] Train loss : [0.025025458740336553] Val Score : [0.9165787375726882])\n",
            "Epoch : [317] Train loss : [0.02203969710639545] Val Score : [0.9165787375726882])\n",
            "Epoch : [318] Train loss : [0.02074520555990083] Val Score : [0.9165787375726882])\n",
            "Epoch : [319] Train loss : [0.023681067994662693] Val Score : [0.9165787375726882])\n",
            "Epoch : [320] Train loss : [0.023033771397812024] Val Score : [0.9165787375726882])\n",
            "Epoch : [321] Train loss : [0.021008573206407682] Val Score : [0.9165787375726882])\n",
            "Epoch : [322] Train loss : [0.022844601954732622] Val Score : [0.9165787375726882])\n",
            "Epoch : [323] Train loss : [0.021436756742852076] Val Score : [0.9165787375726882])\n",
            "Epoch : [324] Train loss : [0.021842419835073606] Val Score : [0.9165787375726882])\n",
            "Epoch : [325] Train loss : [0.02363294575895582] Val Score : [0.9165787375726882])\n",
            "Epoch : [326] Train loss : [0.021768390334078243] Val Score : [0.9165787375726882])\n",
            "Epoch : [327] Train loss : [0.024024864392621175] Val Score : [0.9165787375726882])\n",
            "Epoch : [328] Train loss : [0.022426544821688106] Val Score : [0.9165787375726882])\n",
            "Epoch : [329] Train loss : [0.02174761226134641] Val Score : [0.9165787375726882])\n",
            "Epoch : [330] Train loss : [0.02446530626288482] Val Score : [0.9165787375726882])\n",
            "Epoch : [331] Train loss : [0.022118870967200825] Val Score : [0.9165787375726882])\n",
            "Epoch : [332] Train loss : [0.02185621644769396] Val Score : [0.9165787375726882])\n",
            "Epoch : [333] Train loss : [0.021848803386092186] Val Score : [0.9165787375726882])\n",
            "Epoch : [334] Train loss : [0.02235007498945509] Val Score : [0.9165787375726882])\n",
            "Epoch : [335] Train loss : [0.021565545350313187] Val Score : [0.9165787375726882])\n",
            "Epoch : [336] Train loss : [0.021014433886323656] Val Score : [0.9165787375726882])\n",
            "Epoch : [337] Train loss : [0.02199309239430087] Val Score : [0.9165787375726882])\n",
            "Epoch : [338] Train loss : [0.024317632828439985] Val Score : [0.9165787375726882])\n",
            "Epoch : [339] Train loss : [0.021982943639159203] Val Score : [0.9165787375726882])\n",
            "Epoch : [340] Train loss : [0.022072556827749525] Val Score : [0.9165787375726882])\n",
            "Epoch : [341] Train loss : [0.021309587306209972] Val Score : [0.9165787375726882])\n",
            "Epoch : [342] Train loss : [0.023944648248808726] Val Score : [0.9165787375726882])\n",
            "Epoch : [343] Train loss : [0.02194735700530665] Val Score : [0.9165787375726882])\n",
            "Epoch : [344] Train loss : [0.02122004090675286] Val Score : [0.9165787375726882])\n",
            "Epoch : [345] Train loss : [0.02283908160669463] Val Score : [0.9165787375726882])\n",
            "Epoch : [346] Train loss : [0.021109316231948987] Val Score : [0.9165787375726882])\n",
            "Epoch : [347] Train loss : [0.022722981870174408] Val Score : [0.9165787375726882])\n",
            "Epoch : [348] Train loss : [0.0235047915152141] Val Score : [0.9165787375726882])\n",
            "Epoch : [349] Train loss : [0.0222715756722859] Val Score : [0.9165787375726882])\n",
            "Epoch : [350] Train loss : [0.02170260756143502] Val Score : [0.9165787375726882])\n",
            "Epoch : [351] Train loss : [0.023472926446369717] Val Score : [0.9165787375726882])\n",
            "Epoch : [352] Train loss : [0.023707887956074307] Val Score : [0.9165787375726882])\n",
            "Epoch : [353] Train loss : [0.021517120035631315] Val Score : [0.9165787375726882])\n",
            "Epoch : [354] Train loss : [0.022553973166005954] Val Score : [0.9165787375726882])\n",
            "Epoch : [355] Train loss : [0.0218127899404083] Val Score : [0.9165787375726882])\n",
            "Epoch : [356] Train loss : [0.021405979192682674] Val Score : [0.9165787375726882])\n",
            "Epoch : [357] Train loss : [0.022035768521683558] Val Score : [0.9165787375726882])\n",
            "Epoch : [358] Train loss : [0.02366742811032704] Val Score : [0.9165787375726882])\n",
            "Epoch : [359] Train loss : [0.022707754213895117] Val Score : [0.9165787375726882])\n",
            "Epoch : [360] Train loss : [0.02454054169356823] Val Score : [0.9165787375726882])\n",
            "Epoch : [361] Train loss : [0.022149326811943735] Val Score : [0.9165787375726882])\n",
            "Epoch : [362] Train loss : [0.020834970687116896] Val Score : [0.9165787375726882])\n",
            "Epoch : [363] Train loss : [0.02169466364596571] Val Score : [0.9165787375726882])\n",
            "Epoch : [364] Train loss : [0.02109062618442944] Val Score : [0.9165787375726882])\n",
            "Epoch : [365] Train loss : [0.020849932251232012] Val Score : [0.9165787375726882])\n",
            "Epoch : [366] Train loss : [0.02181495007659708] Val Score : [0.9165787375726882])\n",
            "Epoch : [367] Train loss : [0.021508336599384035] Val Score : [0.9165787375726882])\n",
            "Epoch : [368] Train loss : [0.023704372878585542] Val Score : [0.9165787375726882])\n",
            "Epoch : [369] Train loss : [0.023793968771185194] Val Score : [0.9165787375726882])\n",
            "Epoch : [370] Train loss : [0.0229707873825516] Val Score : [0.9165787375726882])\n",
            "Epoch : [371] Train loss : [0.021238213405013084] Val Score : [0.9165787375726882])\n",
            "Epoch : [372] Train loss : [0.022590872698596547] Val Score : [0.9165787375726882])\n",
            "Epoch : [373] Train loss : [0.02070277663213866] Val Score : [0.9165787375726882])\n",
            "Epoch : [374] Train loss : [0.02289493222321783] Val Score : [0.9165787375726882])\n",
            "Epoch : [375] Train loss : [0.02229913111243929] Val Score : [0.9165787375726882])\n",
            "Epoch : [376] Train loss : [0.022852906957268715] Val Score : [0.9165787375726882])\n",
            "Epoch : [377] Train loss : [0.02335334356342043] Val Score : [0.9165787375726882])\n",
            "Epoch : [378] Train loss : [0.020497967888201987] Val Score : [0.9165787375726882])\n",
            "Epoch : [379] Train loss : [0.021321827545762062] Val Score : [0.9165787375726882])\n",
            "Epoch : [380] Train loss : [0.022505699257765497] Val Score : [0.9165787375726882])\n",
            "Epoch : [381] Train loss : [0.021792707964777946] Val Score : [0.9165787375726882])\n",
            "Epoch : [382] Train loss : [0.0214864228452955] Val Score : [0.9165787375726882])\n",
            "Epoch : [383] Train loss : [0.022540262767246792] Val Score : [0.9165787375726882])\n",
            "Epoch : [384] Train loss : [0.023632650396653583] Val Score : [0.9165787375726882])\n",
            "Epoch : [385] Train loss : [0.023753812270505086] Val Score : [0.9165787375726882])\n",
            "Epoch : [386] Train loss : [0.021675414804901396] Val Score : [0.9165787375726882])\n",
            "Epoch : [387] Train loss : [0.023556773151670183] Val Score : [0.9165787375726882])\n",
            "Epoch : [388] Train loss : [0.02112549383725439] Val Score : [0.9165787375726882])\n",
            "Epoch : [389] Train loss : [0.020871307434780256] Val Score : [0.9165787375726882])\n",
            "Epoch : [390] Train loss : [0.024326829239726067] Val Score : [0.9165787375726882])\n",
            "Epoch : [391] Train loss : [0.02192872736070837] Val Score : [0.9165787375726882])\n",
            "Epoch : [392] Train loss : [0.022683474368282726] Val Score : [0.9165787375726882])\n",
            "Epoch : [393] Train loss : [0.021319989381091937] Val Score : [0.9165787375726882])\n",
            "Epoch : [394] Train loss : [0.021331575033920153] Val Score : [0.9165787375726882])\n",
            "Epoch : [395] Train loss : [0.020206633423055922] Val Score : [0.9165787375726882])\n",
            "Epoch : [396] Train loss : [0.023889714319791113] Val Score : [0.9165787375726882])\n",
            "Epoch : [397] Train loss : [0.021587144849555834] Val Score : [0.9165787375726882])\n",
            "Epoch : [398] Train loss : [0.02099732176533767] Val Score : [0.9165787375726882])\n",
            "Epoch : [399] Train loss : [0.02406328304537705] Val Score : [0.9165787375726882])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoEncoder()"
      ],
      "metadata": {
        "id": "mK-XKvhuSXkP"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('./best_model.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtI5-KvRW3hT",
        "outputId": "bc79d96d-0ba4-47c0-c6ae-b2dcb3c298bc"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.DataParallel(model)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iA2-mLynW-iw",
        "outputId": "fe3327ca-402f-4f50-9e40-93ea8acf5c91"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataParallel(\n",
              "  (module): AutoEncoder(\n",
              "    (Encoder): Sequential(\n",
              "      (0): Linear(in_features=30, out_features=64, bias=True)\n",
              "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): LeakyReLU(negative_slope=0.01)\n",
              "      (3): Linear(in_features=64, out_features=128, bias=True)\n",
              "      (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): LeakyReLU(negative_slope=0.01)\n",
              "    )\n",
              "    (Decoder): Sequential(\n",
              "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
              "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): LeakyReLU(negative_slope=0.01)\n",
              "      (3): Linear(in_features=64, out_features=30, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv('./test.csv')\n",
        "test_df = test_df.drop(columns=['ID'])"
      ],
      "metadata": {
        "id": "O5sV_-YUXCmg"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = MyDataset(test_df, False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BS, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "StazU7d0XF9M"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction(model,thr,test_loader, device):\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "  pred = []\n",
        "  with torch.no_grad():\n",
        "    for x in iter(test_loader):\n",
        "      x = x.float().to(device)\n",
        "\n",
        "      _x = model(x)\n",
        "\n",
        "      diff = cos(x, _x).cpu().tolist()\n",
        "      batch_pred = np.where(np.array(diff)<thr, 1,0).tolist()\n",
        "      pred += batch_pred\n",
        "  return pred\n"
      ],
      "metadata": {
        "id": "Xxds05T5XGul"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = prediction(model, 0.95, test_loader, device)"
      ],
      "metadata": {
        "id": "AU3u-G5LY9sF"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit = pd.read_csv('./sample_submission.csv')\n",
        "submit['Class'] = preds\n",
        "submit.to_csv('./submit_autoencoder.csv', index=False)"
      ],
      "metadata": {
        "id": "bPZUpWMDZAj8"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit['Class'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-e3FHhk2ZC7s",
        "outputId": "ef464488-ff99-4dd5-90ca-de5b86c2f0cf"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u0lGbKAtZDGF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}